{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== IMPORT UTILS ===========\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== IMPORT DATASETS ===========\n",
    "train = pd.read_csv(\"data/train_data_ner.csv\")\n",
    "test = pd.read_csv(\"data/test_data_ner.csv\")\n",
    "tiny_test = pd.read_csv(\"data/tiny_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentence_id     words tags\n",
      "85436        20056       NaN    O\n",
      "85437        20056        of    O\n",
      "85438        20056     those    O\n",
      "85439        20056  released    O\n",
      "85440        20056       was    O\n",
      "85441        20056    guilty    O\n",
      "85442        20056        of    O\n",
      "85443        20056   violent    O\n",
      "85444        20056    crimes    O\n",
      "85445        20056         .    O\n",
      "       sentence_id     words tags\n",
      "85436        20056       nan    O\n",
      "85437        20056        of    O\n",
      "85438        20056     those    O\n",
      "85439        20056  released    O\n",
      "85440        20056       was    O\n",
      "85441        20056    guilty    O\n",
      "85442        20056        of    O\n",
      "85443        20056   violent    O\n",
      "85444        20056    crimes    O\n",
      "85445        20056         .    O\n"
     ]
    }
   ],
   "source": [
    "# =========== NAN VALUES ===========\n",
    "# examples\n",
    "print(test.loc[test.sentence_id==20056])\n",
    "# print(test.loc[test.sentence_id==46902])\n",
    "\n",
    "# change nan's to string\n",
    "train['words'] = train['words'].astype(str)\n",
    "test['words'] = test['words'].astype(str)\n",
    "tiny_test['words'] = tiny_test['words'].astype(str)\n",
    "\n",
    "# train['words'] = np.where(train['words'].isna(), \"none\", train['words'])\n",
    "# test['words'] = np.where(test['words'].isna(), \"none\", test['words'])\n",
    "# tiny_test['words'] = np.where(tiny_test['words'].isna(), \"none\", tiny_test['words'])\n",
    "\n",
    "# fix\n",
    "print(test.loc[test.sentence_id==20056])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating: 100%|██████████| 13/13 [00:00<00:00, 2740.82sentence/s]\n",
      "Creating: 100%|██████████| 38366/38366 [00:18<00:00, 2073.69sentence/s]\n",
      "Creating: 100%|██████████| 38367/38367 [00:18<00:00, 2057.47sentence/s]\n"
     ]
    }
   ],
   "source": [
    "# =========== CONVERT DATASETS ===========\n",
    "\n",
    "# Tiny dataset from the assignment pdf\n",
    "# X_tiny, y_tiny = tiny_test()\n",
    "X_tiny, y_tiny = transform_data_sentence_tag(tiny_test)\n",
    "\n",
    "# Test, train data\n",
    "X_train, y_train = transform_data_sentence_tag(train)\n",
    "X_test, y_test = transform_data_sentence_tag(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== CREATE DICTIONARIES WITH UNIQUE INDEX  ===========\n",
    "\n",
    "# To store values efficiently (INTEGERS/WORDS and INTEGERS/TAGS)\n",
    "# x attribute: list of words (integer words)\n",
    "# y attribute: list of tags (integer tags)\n",
    "# Then we need to keep a mapping from integers to words and from integers to tags.\n",
    "\n",
    "word_dict, tag_dict = create_corpus(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Sequence List: 100%|██████████| 38366/38366 [02:17<00:00, 278.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# =========== CREATE SEQUENCE LIST  ===========\n",
    "\n",
    "# Next we will help ourselves with the nlp_hmms lecture and create a SequenceList object (without cython)\n",
    "sequence_list = create_sequence_list(X_train, y_train, word_dict, tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== CREATE TAGS  ===========\n",
    "\n",
    "train_tag_pos = [[tag_dict[i] for i in tag] for tag in y_train]\n",
    "y_train_true = [tag for array in train_tag_pos for tag in array]\n",
    "\n",
    "test_tag_pos = [[tag_dict[i] for i in tag] for tag in y_test]\n",
    "y_test_true = [tag for array in test_tag_pos for tag in array]\n",
    "\n",
    "tiny_tag_pos = [[tag_dict[i] for i in tag] for tag in y_tiny]\n",
    "y_tiny_true = [tag for array in tiny_tag_pos for tag in array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== MODELS IMPORT  ===========\n",
    "\n",
    "# =========== MODELS - Default Features ===========\n",
    "feature_mapper = IDFeatures(sequence_list)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "structured_perceptron = StructuredPerceptron(word_dict, tag_dict, feature_mapper)\n",
    "structured_perceptron.load_model(dir='fitted_models/default_features_model_wo_cython')\n",
    "\n",
    "# =========== PREDICTION AND EVALUATION TEST SET ===========\n",
    "\n",
    "y_test_pred = []\n",
    "\n",
    "for i in tqdm(range(len(X_test)), desc=\"Predicting tags\", unit=\"sequence\"):\n",
    "    predicted_tag = structured_perceptron.predict_tags_given_words(X_test[i])\n",
    "    y_test_pred.append(predicted_tag)\n",
    "\n",
    "y_test_pred = [np.ndarray.tolist(array) for array in y_test_pred]\n",
    "y_test_pred = np.concatenate(y_test_pred).ravel().tolist()\n",
    "\n",
    "print(f1_score_weighted(y_test_true, y_test_pred))\n",
    "print(accuracy(y_test_true, y_test_pred))\n",
    "plot_confusion_matrix(y_test_true, y_test_pred, tag_dict)\n",
    "\n",
    "# =========== PREDICTION AND EVALUATION TINY SET ===========\n",
    "\n",
    "y_tiny_test_pred = []\n",
    "\n",
    "for i in tqdm(range(len(X_tiny)), desc=\"Predicting tags\", unit=\"sequence\"):\n",
    "    predicted_tag = structured_perceptron.predict_tags_given_words(X_tiny[i])\n",
    "    y_tiny_test_pred.append(predicted_tag)\n",
    "\n",
    "y_tiny_test_pred = [np.ndarray.tolist(array) for array in y_tiny_test_pred]\n",
    "y_tiny_test_pred = np.concatenate(y_tiny_test_pred).ravel().tolist()\n",
    "\n",
    "print(f1_score_weighted(y_tiny_true, y_tiny_test_pred))\n",
    "print(accuracy(y_tiny_true, y_tiny_test_pred))\n",
    "plot_confusion_matrix(y_tiny_true, y_tiny_test_pred, tag_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fitted_models/extra_features_model_wo_cythonparameters.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m extra_mapping_feature\u001b[39m.\u001b[39mbuild_features()\n\u001b[1;32m      7\u001b[0m structured_perceptron_extraf \u001b[39m=\u001b[39m spc\u001b[39m.\u001b[39mStructuredPerceptron(word_dict, tag_dict, extra_mapping_feature)\n\u001b[0;32m----> 8\u001b[0m structured_perceptron_extraf\u001b[39m.\u001b[39mload_model(\u001b[39mdir\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfitted_models/extra_features_model_wo_cython\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# =========== PREDICTION AND EVALUATION TEST SET ===========\u001b[39;00m\n\u001b[1;32m     12\u001b[0m y_test_pred_1 \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/NLP_Assignment_2/Ignacio_Zemencikova/skseq/sequences/structured_perceptron.py:183\u001b[0m, in \u001b[0;36mStructuredPerceptron.load_model\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mdir\u001b[39m):\n\u001b[1;32m    180\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39m    Loads the parameters of the model\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     fn \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mdir\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mparameters.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    184\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m fn:\n\u001b[1;32m    185\u001b[0m         toks \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fitted_models/extra_features_model_wo_cythonparameters.txt'"
     ]
    }
   ],
   "source": [
    "# =========== MODELS - Added Features ===========\n",
    "from skseq.sequences import extended_feature\n",
    "import skseq.sequences.structured_perceptron as spc\n",
    "extra_mapping_feature = extended_feature.ExtendedFeatures(sequence_list) \n",
    "extra_mapping_feature.build_features()\n",
    "\n",
    "structured_perceptron_extraf = spc.StructuredPerceptron(word_dict, tag_dict, extra_mapping_feature)\n",
    "structured_perceptron_extraf.load_model(dir='fitted_models/extra_features_model_wo_cython')\n",
    "\n",
    "# =========== PREDICTION AND EVALUATION TEST SET ===========\n",
    "\n",
    "y_test_pred_1 = []\n",
    "\n",
    "for i in tqdm(range(len(X_test)), desc=\"Predicting tags\", unit=\"sequence\"):\n",
    "    predicted_tag = structured_perceptron_extraf.predict_tags_given_words(X_test[i])\n",
    "    y_test_pred_1.append(predicted_tag)\n",
    "\n",
    "y_test_pred_1 = [np.ndarray.tolist(array) for array in y_test_pred_1]\n",
    "y_test_pred_1 = np.concatenate(y_test_pred_1).ravel().tolist()\n",
    "\n",
    "print(f1_score_weighted(y_test_true, y_test_pred_1))\n",
    "print(accuracy(y_test_true, y_test_pred_1))\n",
    "plot_confusion_matrix(y_test_true, y_test_pred_1, tag_dict)\n",
    "\n",
    "# =========== PREDICTION AND EVALUATION TINY SET ===========\n",
    "\n",
    "y_tiny_test_pred_1 = []\n",
    "\n",
    "for i in tqdm(range(len(X_tiny)), desc=\"Predicting tags\", unit=\"sequence\"):\n",
    "    predicted_tag = structured_perceptron_extraf.predict_tags_given_words(X_tiny[i])\n",
    "    y_tiny_test_pred_1.append(predicted_tag)\n",
    "\n",
    "y_tiny_test_pred_1 = [np.ndarray.tolist(array) for array in y_tiny_test_pred_1]\n",
    "y_tiny_test_pred_1 = np.concatenate(y_tiny_test_pred_1).ravel().tolist()\n",
    "\n",
    "print(f1_score_weighted(y_tiny_true, y_tiny_test_pred_1))\n",
    "print(accuracy(y_tiny_true, y_tiny_test_pred_1))\n",
    "plot_confusion_matrix(y_tiny_true, y_tiny_test_pred_1, tag_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
